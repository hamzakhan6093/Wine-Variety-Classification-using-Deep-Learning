{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd3919c-2e19-499a-b2d8-0969ac7cdad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Assignment Data.csv', encoding='latin1')\n",
    "\n",
    "# Data Cleaning\n",
    "df['description'] = df['description'].fillna('')\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply text preprocessing to features\n",
    "df['clean_description'] = df['description'].apply(preprocess_text)\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['variety'])\n",
    "y = to_categorical(y_encoded)  # For multi-class classification\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf.fit_transform(df['clean_description']).toarray()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca00a15a-a6ae-4957-a3d3-e060c5b02dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0)),\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))  # Output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "910b8c3c-afad-4c93-a1f0-4a6b7e2b8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be3392f2-3944-4ef1-a489-e68b9311c3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.1465 - loss: 4.0335 - val_accuracy: 0.3868 - val_loss: 2.5983\n",
      "Epoch 2/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.3938 - loss: 2.5816 - val_accuracy: 0.4546 - val_loss: 2.2542\n",
      "Epoch 3/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.4547 - loss: 2.2610 - val_accuracy: 0.4793 - val_loss: 2.1170\n",
      "Epoch 4/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.4877 - loss: 2.0702 - val_accuracy: 0.5021 - val_loss: 2.0292\n",
      "Epoch 5/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.5162 - loss: 1.9524 - val_accuracy: 0.5190 - val_loss: 1.9848\n",
      "Epoch 6/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.5412 - loss: 1.8344 - val_accuracy: 0.5270 - val_loss: 1.9498\n",
      "Epoch 7/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.5561 - loss: 1.7565 - val_accuracy: 0.5289 - val_loss: 1.9344\n",
      "Epoch 8/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.5751 - loss: 1.6752 - val_accuracy: 0.5393 - val_loss: 1.9198\n",
      "Epoch 9/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.5867 - loss: 1.6261 - val_accuracy: 0.5418 - val_loss: 1.9146\n",
      "Epoch 10/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.5958 - loss: 1.5774 - val_accuracy: 0.5415 - val_loss: 1.9134\n",
      "Epoch 11/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.6073 - loss: 1.5210 - val_accuracy: 0.5473 - val_loss: 1.9103\n",
      "Epoch 12/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.6150 - loss: 1.4915 - val_accuracy: 0.5475 - val_loss: 1.9251\n",
      "Epoch 13/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.6216 - loss: 1.4577 - val_accuracy: 0.5480 - val_loss: 1.9214\n",
      "Epoch 14/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.6339 - loss: 1.4195 - val_accuracy: 0.5486 - val_loss: 1.9240\n",
      "Epoch 15/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.6366 - loss: 1.3849 - val_accuracy: 0.5461 - val_loss: 1.9373\n",
      "Epoch 16/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 23ms/step - accuracy: 0.6427 - loss: 1.3646 - val_accuracy: 0.5492 - val_loss: 1.9467\n",
      "Epoch 17/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.6520 - loss: 1.3320 - val_accuracy: 0.5516 - val_loss: 1.9399\n",
      "Epoch 18/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.6580 - loss: 1.3103 - val_accuracy: 0.5509 - val_loss: 1.9553\n",
      "Epoch 19/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.6575 - loss: 1.3015 - val_accuracy: 0.5512 - val_loss: 1.9660\n",
      "Epoch 20/20\n",
      "\u001b[1m650/650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.6673 - loss: 1.2696 - val_accuracy: 0.5519 - val_loss: 1.9894\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bdff048-9c43-4a87-a1e9-86bc68c0c85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5588 - loss: 1.9777\n",
      "Test Loss: 2.0055, Test Accuracy: 0.5552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50e97394-01e5-464b-b267-db4f3fd3f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction function for new data\n",
    "def predict_variety(new_description):\n",
    "    cleaned_text = preprocess_text(new_description)\n",
    "    vectorized = tfidf.transform([cleaned_text]).toarray()\n",
    "    prediction = model.predict(vectorized)\n",
    "    return le.inverse_transform([np.argmax(prediction)])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea05593c-ee16-4de3-bf76-6353bebf1e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "For Example: A full-bodied red with dark berry flavors and hints of oak\n",
      "\t     A full-bodied red with dark berry flavors and hints of oak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Predicted Wine Variety: Pinot Noir\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example prediction\n",
    "new_wine_description = input(\"For Example: A full-bodied red with dark berry flavors and hints of oak\\n\" + \"\\t    \")\n",
    "predicted_variety = predict_variety(new_wine_description)\n",
    "print(f\"Predicted Wine Variety: {predicted_variety}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86fd63-af1c-4b6a-b413-4b360fb92019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
